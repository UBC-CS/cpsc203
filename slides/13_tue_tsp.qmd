---
date: 2025-11-25
---

## Announcements

# Travelling Salesperson Problem

## TSP vs SSSP {data-background-image="/images/maps1data.png"}

SSSP: ‚Äúmany one-way routes from home (tree)‚Äù vs TSP: ‚Äúone big round trip through everyone (loop)‚Äù

## TSP vs SSSP {.smaller}

| Aspect | SSSP (Single-Source Shortest Paths) | TSP (Travelling Salesperson) |
|--------|-------------------------------------|------------------------------|
| Goal | Shortest route from one start to **each** vertex | One **cheapest tour** visiting all vertices and returning |
| Shape | **Tree** of paths from the source | **Cycle** through all vertices |
| Must visit all? | No ‚Äì each path can **skip** most vertices | Yes ‚Äì must **visit every** vertex exactly once |
| Overlap | Paths often **share edges** | Tour is one sequence; no repeated visits |
| Difficulty | Fast algorithms (e.g., Dijkstra) | NP-hard; exact solution is hard for large graphs |


## Demo Blog

<https://medium.com/data-science/around-the-world-in-90-414-kilometers-ce84c03b8552>

![](/images/maps9caps.png)

## Plan for Code {data-background-image="/images/maps9chart.png"}

Steps to assemble our solution:

\ 

1. \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

2. \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

3. \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

4. \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

5. \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

::: notes
1. assemble the graph and errands
2. create pairwise distances
3. create all possible permutations of paths
4. find minimum route and assemble turn-by-turn for that route
5. visualize on map
:::

## A Technical Detail

![](/images/maps9home.png)

## Demo {.activity}

[PrairieLearn Activity](https://us.prairielearn.com/pl/course_instance/193300/assessment/2616092)

## What Does `lambda` Do? {.smaller}

``` python
dferrands['latlong'] = dferrands.apply(
                lambda row: ox.geocode(row['errand']),
                axis=1
              )
```

**Goal:** Create a new column (`latlong`) and fill it with coordinates for each errand in the dataframe.

**How it works:**

* `df.apply(..., axis=1)`:  Run a function once for each row.

* `lambda row: ox.geocode(row['errand'])`: A tiny inline function. Read it as:
"Given a row, look up the place name in `row['errand']`,geocode it, and return the (lat, lon)."

* The returned value becomes the entry in the new latlong column.

**Why lambda?** We only need this function once, so we write it right where it‚Äôs used.

## Tour Distance

We assemble each candidate solution as an arrangement of all of the errands:

Ex: `A D B E F C`

How do we find the total distance if we do the errands in the suggested order?

## Why This Matters {.smaller}

There are *lots* of applications:

- üì¶ **UPS/FedEx/Amazon Delivery** Millions saved by reducing even 1km per driver per day.
- üçî **UberEats/DoorDash/Skip** Multi-stop pickup/delivery with time windows.
- üß¨ **DNA Sequencing & Genome Assembly** Ordering fragments relies on TSP-like reconstruction.
- üöå **School Bus Routing** Minimize buses, fuel, and time while meeting constraints.
- üß™ **Medical Lab Sample Pipelines** Robotic arms schedule efficient multi-station workflows.
- ‚ùÑÔ∏è **Snowplow & Garbage Truck Routing** Efficiently cover every street in a city.
- üè≠ **Factory Robots & CNC Machines** Optimizing tool paths reduces waste, heat, and wear.
- ‚úàÔ∏è **Airline Crew Scheduling** Complex daily routing: crews must return to base, meet rest rules.
- üåç **City Infrastructure Planning** Utility inspections, meter reading, streetlight repair routes.
- üöë **Ambulance & Emergency Routing** Minimize response times; life-critical optimization.

# Text as Data

## Visualizing Literature

![](/images/vis1.png)

## Lexical Novelty {.smaller}

::::: columns 
::: {.column}
![](/images/vis2.png)
:::
::: {.column}
"Novels are full of new characters, new locations and new expressions. The discourse between characters involves new ideas being exchanged. We can get a hint of this by tracking the introduction of new terms in a novel. In the below visualizations (in which each column represents a chapter and each small block a paragraph of text), I maintain a variable which represents novelty. When a paragraph contains more than 25% new terms (i.e. words that have not been observed thus far) this variable is set at its maximum of 1. Otherwise, the variable decays. The variable is used to colour the paragraph with red being 1.0 and blue being 0. The result is that we can get an idea of the introduction of new ideas in novels."  - Matthew Hurst
:::
:::::




## Word Co-occurrence 

![](/images/vis3.png)

## Cross References

![](/images/vis4.png)

## Sentiment Analysis

![](/images/vis5.png)

## Mapping

![](/images/vis6.png)


## Visualizing Literature

- A novel is just a **long sequence of characters**.
- If we treat it as **data**, we can:
  - Count and visualize patterns,
  - Compare books or authors,
  - Build networks of characters and concepts.
- Today: turn a novel into **numbers and pictures**.

## What Can We Compute From a Book? {.activity}

If I give you the full text of a novel as a `.txt` file

  - What **questions** could we ask?
  - What **numbers** or **graphs** could we compute?

Ideas:

## The Text-as-Data Pipeline {.smaller}

**Data flow:**

1. **Load** the text from a file
2. **Tokenize**: split into words
3. **Preprocess**:
   - lowercase
   - remove punctuation
   - remove stopwords
   - use **lemmas**
4. **Count** frequencies
5. **Visualize** as a bar chart
6. **Interpret**: what do the counts tell us?

## Step 1: Getting the Text into Python

``` {pyodide}
import requests

url = "https://raw.githubusercontent.com/UBC-CS/cpsc203/main/data/ofk.txt"

resp = requests.get(url)
resp.raise_for_status()   # optional but good practice
text = resp.text          # this is your whole file as one big string

print(text[:500])  # peek at the first 500 characters
```


## Tokenizer

We want to analyze the data by word or by \_\_\_\_\_\_\_\_\_\_\_\_ or by \_\_\_\_\_\_\_\_\_\_\_\_ or by \_\_\_\_\_\_\_\_\_\_\_\_...  

We can separate the data (string) into any of these using a ‚Äútokenizer‚Äù


## Tokenization

\ 

Translate: ‚ÄúAstrology. The governess was always\ngetting muddled with her astrolabe, and when she got specially muddled she would take it out\nof the Wart by rapping his knuckles. She did not rap Kay's knuckles, because when Kay grew\nolder‚Äù

\ 

Into: ['Astrology.', 'The', 'governess', 'was', 'always', 'getting', 'muddled', 'with', 'her', 'astrolabe', ',', 'and', 'when', 'she', 'got', 'specially', 'muddled', 'she', 'would', 'take', 'it', 'out', 'of', 'the', 'Wart', 'by', 'rapping', 'his', 'knuckles.', 'She', 'did', 'not', 'rap', 'Kay', "'s", 'knuckles', ',', 'because', 'when', 'Kay', 'grew', 'older']

## Tokenization

Quick, but not very smart:

``` {pyodide}

words = text.split()
print(words[:20])

```









