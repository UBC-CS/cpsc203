---
date: 2025-11-27
---

## Announcements

# Text as Data

## Visualizing Literature

![](/images/vis1.png)

## Lexical Novelty {.smaller}

::::: columns 
::: {.column}
![](/images/vis2.png)
:::
::: {.column}
"Novels are full of new characters, new locations and new expressions. The discourse between characters involves new ideas being exchanged. We can get a hint of this by tracking the introduction of new terms in a novel. In the below visualizations (in which each column represents a chapter and each small block a paragraph of text), I maintain a variable which represents novelty. When a paragraph contains more than 25% new terms (i.e. words that have not been observed thus far) this variable is set at its maximum of 1. Otherwise, the variable decays. The variable is used to colour the paragraph with red being 1.0 and blue being 0. The result is that we can get an idea of the introduction of new ideas in novels."  - Matthew Hurst
:::
:::::




## Word Co-occurrence 

![](/images/vis3.png)

## Cross References

![](/images/vis4.png)

## Sentiment Analysis

![](/images/vis5.png)

## Mapping

![](/images/vis6.png)


## Visualizing Literature

- A novel is just a **long sequence of characters**.
- If we treat it as **data**, we can:
  - Count and visualize patterns,
  - Compare books or authors,
  - Build networks of characters and concepts.
- Today: turn a novel into **numbers and pictures**.

## What Can We Compute From a Book? {.activity}

If I give you the full text of a novel as a `.txt` file

  - What **questions** could we ask?
  - What **numbers** or **graphs** could we compute?

Ideas:

## The Text-as-Data Pipeline {.smaller}

**Data flow:**

1. **Load** the text from a file
2. **Tokenize**: split into words
3. **Preprocess**:
   - lowercase
   - remove punctuation
   - remove stopwords
   - use **lemmas**
4. **Count** frequencies
5. **Visualize** as a bar chart
6. **Interpret**: what do the counts tell us?

## Step 1: Getting the Text into Python

``` {pyodide}
import requests

url = "https://raw.githubusercontent.com/UBC-CS/cpsc203/main/data/ofk.txt"

resp = requests.get(url)
resp.raise_for_status()   # optional but good practice
text = resp.text          # this is your whole file as one big string

print(text[:500])  # peek at the first 500 characters
```


## Tokenizer

We want to analyze the data by word or by \_\_\_\_\_\_\_\_\_\_\_\_ or by \_\_\_\_\_\_\_\_\_\_\_\_ or by \_\_\_\_\_\_\_\_\_\_\_\_...  

We can separate the data (string) into any of these using a “tokenizer”


## Tokenization

\ 

Translate: “Astrology. The governess was always\ngetting muddled with her astrolabe, and when she got specially muddled she would take it out\nof the Wart by rapping his knuckles. She did not rap Kay's knuckles, because when Kay grew\nolder”

\ 

Into: ['Astrology.', 'The', 'governess', 'was', 'always', 'getting', 'muddled', 'with', 'her', 'astrolabe', ',', 'and', 'when', 'she', 'got', 'specially', 'muddled', 'she', 'would', 'take', 'it', 'out', 'of', 'the', 'Wart', 'by', 'rapping', 'his', 'knuckles.', 'She', 'did', 'not', 'rap', 'Kay', "'s", 'knuckles', ',', 'because', 'when', 'Kay', 'grew', 'older']

## Tokenization

Quick, but not very smart:

``` {pyodide}

words = text.split()
print(words[:20])

```

## SpaCy Tokenizer

``` {pyodide}
import spacy
nlp = spacy.load("en_core_web_sm")

doc = nlp(text)
tokens = [t.text for t in doc]
print(tokens[:30])
```








